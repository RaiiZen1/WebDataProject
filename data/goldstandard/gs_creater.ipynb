{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ws_tax_hiwi\\AppData\\Local\\Temp\\12\\ipykernel_107660\\3374035507.py:5: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  covers = pd.read_csv(\"data\\Books.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "amazon = pd.read_csv('data\\main_dataset.csv')\n",
    "goodreads = pd.read_csv(\"data\\goodreads_books.csv\")\n",
    "covers = pd.read_csv(\"data\\Books.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11882 duplicates in the goodreads dataset\n",
      "There are 9873 duplicates in the amazon dataset\n",
      "There are 29225 duplicates in the covers dataset\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in title column\n",
    "a = goodreads.isbn.duplicated().sum()\n",
    "b = amazon.isbn.duplicated().sum()\n",
    "c = covers[\"Book-Title\"].duplicated().sum()\n",
    "\n",
    "print(\"There are {} duplicates in the goodreads dataset\".format(a))\n",
    "print(\"There are {} duplicates in the amazon dataset\".format(b))\n",
    "print(\"There are {} duplicates in the covers dataset\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicates in the goodreads dataset\n",
    "goodreads = goodreads.dropna(subset=['isbn'])\n",
    "# Drop rows that have the same isbn and keep only the first in the amazon dataset\n",
    "amazon = amazon.drop_duplicates(subset=['isbn'], keep='first')\n",
    "# Drop duplicates in the goodreads dataset\n",
    "goodreads = goodreads.dropna(subset=['isbn'])\n",
    "# Drop rows that have the same isbn and keep only the first in the amazon dataset\n",
    "amazon = amazon.drop_duplicates(subset=['isbn'], keep='first')\n",
    "# Drop rows that have the same title and keep only those that have a corresponding isbn in the goodreads dataset otherwise keep the first in the covers dataset\n",
    "# Identify duplicate titles in the covers dataset\n",
    "# Step 1: Identify duplicates by title in covers\n",
    "dup_titles_covers = covers[covers[\"Book-Title\"].duplicated(keep=False)]\n",
    "\n",
    "# Step 2 & 3: Check for matching ISBNs and prioritize entries\n",
    "filtered_covers = pd.DataFrame()\n",
    "for title, group in dup_titles_covers.groupby('Book-Title'):\n",
    "    # Check if any ISBN in this group matches those in goodreads\n",
    "    matching_isbns = group[group['ISBN'].isin(goodreads['isbn'])]\n",
    "    \n",
    "    if not matching_isbns.empty:\n",
    "        # Keep entries with matching ISBNs\n",
    "        filtered_covers = pd.concat([filtered_covers, matching_isbns], ignore_index=True)\n",
    "    else:\n",
    "        # Keep the first entry based on index if no matching ISBNs\n",
    "        filtered_covers = pd.concat([filtered_covers, group.head(1)], ignore_index=True)\n",
    "\n",
    "# Step 4: Remove all duplicates from covers\n",
    "covers_df_no_duplicates = covers.drop(dup_titles_covers.index)\n",
    "\n",
    "# Step 5: Concatenate the kept entries back to the covers dataset\n",
    "covers = pd.concat([covers_df_no_duplicates, filtered_covers], ignore_index=True)\n",
    "\n",
    "# Verify the result\n",
    "covers[\"Book-Title\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def valid_isbn10(isbn):\n",
    "    \"\"\"Check if the provided string is a valid ISBN-10.\"\"\"\n",
    "    if len(isbn) != 10:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        total = sum((10 - i) * (int(x) if x != 'X' else 10) for i, x in enumerate(isbn))\n",
    "        return total % 11 == 0\n",
    "    except ValueError:\n",
    "        # A ValueError will be raised if a character other than a digit or 'X' is encountered.\n",
    "        return False\n",
    "    \n",
    "def valid_isbn13(isbn):\n",
    "    \"\"\"Check if the provided string is a valid ISBN-13.\"\"\"\n",
    "    if len(isbn) != 13 or not isbn.isdigit():\n",
    "        return False\n",
    "\n",
    "    total = 0\n",
    "    for i in range(12):\n",
    "        if i % 2 == 0:\n",
    "            total += int(isbn[i])\n",
    "        else:\n",
    "            total += 3 * int(isbn[i])\n",
    "\n",
    "    check_digit = 10 - (total % 10)\n",
    "    if check_digit == 10:\n",
    "        check_digit = 0\n",
    "\n",
    "    return str(check_digit) == isbn[-1]\n",
    "\n",
    "def cleanse_dataframe(df, isbn_column):\n",
    "    \"\"\"Return a DataFrame containing only valid ISBN-10s in the specified column.\"\"\"\n",
    "    # Convert the ISBN column to strings\n",
    "    df[isbn_column] = df[isbn_column].astype(str)\n",
    "    # Apply the valid_isbn10 function\n",
    "    return df[df[isbn_column].apply(valid_isbn10)]\n",
    "\n",
    "def cleanse_dataframe_isbn13(df, isbn_column):\n",
    "    \"\"\"Return a DataFrame containing only valid ISBN-13s in the specified column.\"\"\"\n",
    "    df[isbn_column] = df[isbn_column].astype(str)\n",
    "    return df[df[isbn_column].apply(valid_isbn13)]\n",
    "\n",
    "def isbn10_to_isbn13(isbn10):\n",
    "    if len(isbn10) != 10:\n",
    "        return None\n",
    "\n",
    "    # Add the 978 prefix\n",
    "    isbn13 = \"978\" + isbn10[:-1]\n",
    "\n",
    "    # Calculate the checksum for ISBN-13\n",
    "    checksum = 0\n",
    "    for i, char in enumerate(isbn13):\n",
    "        if i % 2 == 0:\n",
    "            checksum += int(char)\n",
    "        else:\n",
    "            checksum += 3 * int(char)\n",
    "\n",
    "    checksum = 10 - (checksum % 10)\n",
    "    if checksum == 10:\n",
    "        checksum = 0\n",
    "\n",
    "    return isbn13 + str(checksum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 307 invalid ISBN-10s in the goodreads dataset\n",
      "There are 866 invalid ISBN-13s in the goodreads dataset\n",
      "There are 0 invalid ISBN-13s in the amazon dataset\n",
      "There are 185 invalid ISBN-10s in the covers dataset\n"
     ]
    }
   ],
   "source": [
    "# Print sum of invalid ISBN-10s in the goodreads dataset\n",
    "print(\"There are {} invalid ISBN-10s in the goodreads dataset\".format(len(goodreads) - len(cleanse_dataframe(goodreads, 'isbn'))))\n",
    "print(\"There are {} invalid ISBN-13s in the goodreads dataset\".format(len(goodreads) - len(cleanse_dataframe_isbn13(goodreads, 'isbn13'))))\n",
    "# Print sum of invalid ISBN-10s in the amazon dataset\n",
    "print(\"There are {} invalid ISBN-13s in the amazon dataset\".format(len(amazon) - len(cleanse_dataframe_isbn13(amazon, 'isbn'))))\n",
    "# Print sum of invalid ISBN-10s and 13s in the covers dataset\n",
    "print(\"There are {} invalid ISBN-10s in the covers dataset\".format(len(covers) - len(cleanse_dataframe(covers, 'ISBN'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goodreads dataset now has 39360 rows\n",
      "The amazon dataset now has 22708 rows\n",
      "The covers dataset now has 241972 rows\n"
     ]
    }
   ],
   "source": [
    "# Drop invalid ISBN-10s in the goodreads dataset\n",
    "goodreads = cleanse_dataframe(goodreads, 'isbn')\n",
    "# Drop invalid ISBN-13s in the goodreads dataset\n",
    "goodreads = cleanse_dataframe_isbn13(goodreads, 'isbn13')\n",
    "# Drop invalid ISBN-13s in the amazon dataset\n",
    "amazon = cleanse_dataframe_isbn13(amazon, 'isbn')\n",
    "# Drop invalid ISBN-10s in the covers dataset\n",
    "covers = cleanse_dataframe(covers, 'ISBN')\n",
    "# Transform covers ISBN to isbn13\n",
    "covers['isbn13'] = covers['ISBN'].apply(isbn10_to_isbn13)\n",
    "covers['isbn13'] = covers['isbn13'].astype('int64')\n",
    "\n",
    "# Print lengths of the datasets\n",
    "print(\"The goodreads dataset now has {} rows\".format(len(goodreads)))\n",
    "print(\"The amazon dataset now has {} rows\".format(len(amazon)))\n",
    "print(\"The covers dataset now has {} rows\".format(len(covers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleansed datasets to csv\n",
    "goodreads.to_csv('data/results/goodreads_clean.csv', index=False)\n",
    "amazon.to_csv('data/results/amazon_clean.csv', index=False)\n",
    "covers.to_csv('data/results/covers_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazon_id</th>\n",
       "      <th>goodreads_id</th>\n",
       "      <th>amazon_title</th>\n",
       "      <th>goodreads_title</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>amazon_isbn</th>\n",
       "      <th>goodreads_isbn</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon_17525</td>\n",
       "      <td>Goodreads_books_32365</td>\n",
       "      <td>The Book of Life</td>\n",
       "      <td>The Book of Life</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9780670025596</td>\n",
       "      <td>9780670025596</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon_14667</td>\n",
       "      <td>Goodreads_books_31157</td>\n",
       "      <td>Winterdance: the Fine Madness of Running the I...</td>\n",
       "      <td>Winterdance: The Fine Madness of Running the I...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9780156001458</td>\n",
       "      <td>9780156001458</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon_21077</td>\n",
       "      <td>Goodreads_books_32845</td>\n",
       "      <td>Stalking Jack the Ripper</td>\n",
       "      <td>Stalking Jack the Ripper</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9780316273497</td>\n",
       "      <td>9780316273497</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon_10466</td>\n",
       "      <td>Goodreads_books_4196</td>\n",
       "      <td>Blue Exorcist, Vol. 3</td>\n",
       "      <td>Blue Exorcist, Vol. 1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9781421540344</td>\n",
       "      <td>9781421540320</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon_7679</td>\n",
       "      <td>Goodreads_books_28881</td>\n",
       "      <td>Reunion in Death</td>\n",
       "      <td>Reunion in Death</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9780425183977</td>\n",
       "      <td>9780749934408</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      amazon_id           goodreads_id  \\\n",
       "0  Amazon_17525  Goodreads_books_32365   \n",
       "1  Amazon_14667  Goodreads_books_31157   \n",
       "2  Amazon_21077  Goodreads_books_32845   \n",
       "3  Amazon_10466   Goodreads_books_4196   \n",
       "4   Amazon_7679  Goodreads_books_28881   \n",
       "\n",
       "                                        amazon_title  \\\n",
       "0                                   The Book of Life   \n",
       "1  Winterdance: the Fine Madness of Running the I...   \n",
       "2                           Stalking Jack the Ripper   \n",
       "3                              Blue Exorcist, Vol. 3   \n",
       "4                                   Reunion in Death   \n",
       "\n",
       "                                     goodreads_title  similarity_score  \\\n",
       "0                                   The Book of Life               1.0   \n",
       "1  Winterdance: The Fine Madness of Running the I...               1.0   \n",
       "2                           Stalking Jack the Ripper               1.0   \n",
       "3                              Blue Exorcist, Vol. 1               1.0   \n",
       "4                                   Reunion in Death               1.0   \n",
       "\n",
       "     amazon_isbn  goodreads_isbn  is_match  \n",
       "0  9780670025596   9780670025596      True  \n",
       "1  9780156001458   9780156001458      True  \n",
       "2  9780316273497   9780316273497      True  \n",
       "3  9781421540344   9781421540320     False  \n",
       "4  9780425183977   9780749934408     False  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleansed datasets\n",
    "amazon_df = pd.read_csv('data/results/amazon_clean.csv')\n",
    "goodreads_df = pd.read_csv('data/results/goodreads_clean.csv')\n",
    "amazon_df[\"id\"] = 'Amazon_' + amazon_df.index.astype(str)\n",
    "goodreads_df['id'] = 'Goodreads_books_' + goodreads_df.index.astype(str)\n",
    "\n",
    "\n",
    "# Preprocessing the title columns\n",
    "amazon_titles = amazon_df['name']\n",
    "goodreads_titles = goodreads_df['title']\n",
    "\n",
    "# Combining the titles from both datasets for TF-IDF vectorization\n",
    "combined_titles = pd.concat([amazon_titles, goodreads_titles])\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(combined_titles)\n",
    "\n",
    "# Splitting the TF-IDF matrix back into two separate matrices for Amazon and Goodreads\n",
    "split_index = len(amazon_titles)\n",
    "tfidf_amazon = tfidf_matrix[:split_index]\n",
    "tfidf_goodreads = tfidf_matrix[split_index:]\n",
    "\n",
    "# Calculating the cosine similarity between every Amazon title and Goodreads title\n",
    "cosine_similarities = cosine_similarity(tfidf_amazon, tfidf_goodreads)\n",
    "\n",
    "# Function to find the best match for each Amazon title in the Goodreads dataset\n",
    "def find_best_matches(cosine_similarities, threshold=0.0):\n",
    "    match_results = []\n",
    "\n",
    "    for i, row in enumerate(cosine_similarities):\n",
    "        best_match_index = np.argmax(row)\n",
    "        best_match_score = row[best_match_index]\n",
    "\n",
    "        if best_match_score > threshold:\n",
    "            amazon_id = amazon_df.iloc[i]['id']  # Get Amazon ID\n",
    "            goodreads_id = goodreads_df.iloc[best_match_index]['id']  # Get Goodreads ID\n",
    "            amazon_isbn = amazon_df.iloc[i]['isbn']\n",
    "            goodreads_isbn = goodreads_df.iloc[best_match_index]['isbn13']\n",
    "            is_match = amazon_isbn == goodreads_isbn\n",
    "            match_results.append({\n",
    "                'amazon_id': amazon_id,\n",
    "                'goodreads_id': goodreads_id,\n",
    "                'amazon_title': amazon_df.iloc[i]['name'],\n",
    "                'goodreads_title': goodreads_df.iloc[best_match_index]['title'],\n",
    "                'similarity_score': best_match_score,\n",
    "                'amazon_isbn': amazon_isbn,\n",
    "                'goodreads_isbn': goodreads_isbn,\n",
    "                'is_match': is_match,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(match_results)\n",
    "\n",
    "# Finding the best matches\n",
    "matched_df = find_best_matches(cosine_similarities)\n",
    "\n",
    "# Filtering to keep only the highest similarity score match for each ISBN\n",
    "final_matched_df = matched_df.sort_values(by='similarity_score', ascending=False)\\\n",
    "                            .drop_duplicates(subset=['amazon_isbn', 'goodreads_isbn'])\\\n",
    "                            .reset_index(drop=True)\n",
    "\n",
    "final_matched_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ws_tax_hiwi\\AppData\\Local\\Temp\\12\\ipykernel_107660\\3615264876.py:4: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  covers_sample = pd.read_csv('data/results/covers_clean.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the reduced covers_sample: 49564\n",
      "Number of overlapping entries with Goodreads: 8071\n",
      "Number of overlapping entries with Amazon: 1246\n"
     ]
    }
   ],
   "source": [
    "# Load the cleansed datasets\n",
    "amazon_sample = pd.read_csv('data/results/amazon_clean.csv')\n",
    "goodreads_sample = pd.read_csv('data/results/goodreads_clean.csv')\n",
    "covers_sample = pd.read_csv('data/results/covers_clean.csv')\n",
    "\n",
    "# Step 1: Identify Overlapping ISBNs\n",
    "overlap_with_goodreads = pd.merge(covers_sample, goodreads_sample, on='isbn13', how='inner')\n",
    "# Assuming a direct comparison is possible here for simplicity:\n",
    "overlap_with_amazon = pd.merge(covers_sample, amazon_sample, left_on='isbn13', right_on='isbn', how='inner')\n",
    "# Combine the overlapping ISBNs\n",
    "overlapping_isbns = pd.concat([overlap_with_goodreads, overlap_with_amazon]).drop_duplicates()\n",
    "# Step 2: Separate Overlapping and Non-Overlapping Data\n",
    "non_overlapping_covers = covers_sample[~covers_sample['isbn13'].isin(overlapping_isbns['isbn13'])]\n",
    "# Step 3: Randomly Sample from Non-Overlapping Data\n",
    "required_sample_size = 50000 - len(overlapping_isbns)\n",
    "additional_samples = non_overlapping_covers.sample(n=required_sample_size, random_state=1)\n",
    "# Combine the overlapping data with the randomly sampled non-overlapping data\n",
    "reduced_covers_sample = pd.concat([overlapping_isbns, additional_samples]).drop_duplicates()\n",
    "# Drop Duplicates of Titles\n",
    "reduced_covers_sample = reduced_covers_sample.drop_duplicates(subset='Book-Title', keep='first')\n",
    "# Check the final size of reduced_covers_sample\n",
    "print(f'Size of the reduced covers_sample: {len(reduced_covers_sample)}')\n",
    "\n",
    "# Checking overlap with goodreads_sample\n",
    "overlap_with_goodreads = pd.merge(reduced_covers_sample, goodreads_sample, on='isbn13', how='inner')\n",
    "# Checking overlap with amazon_sample\n",
    "overlap_with_amazon = pd.merge(reduced_covers_sample, amazon_sample, left_on='isbn13', right_on='isbn', how='inner')\n",
    "# Printing the number of overlapping entries\n",
    "print(f'Number of overlapping entries with Goodreads: {len(overlap_with_goodreads)}')\n",
    "print(f'Number of overlapping entries with Amazon: {len(overlap_with_amazon)}')\n",
    "\n",
    "reduced_covers_sample.to_csv('data/results/covers_clean_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ws_tax_hiwi\\AppData\\Local\\Temp\\12\\ipykernel_107660\\2736687002.py:7: DtypeWarning: Columns (10,11,12,13,14,15,25,26,27,28,29,31,32,33,34,35,36,37,38,39,40,41,43,44,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  covers_df = pd.read_csv(\"data/results/covers_clean_filtered.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>covers_id</th>\n",
       "      <th>goodreads_id</th>\n",
       "      <th>covers_title</th>\n",
       "      <th>goodreads_title</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>covers_isbn</th>\n",
       "      <th>goodreads_isbn</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Covers_4541</td>\n",
       "      <td>Goodreads_books_24907</td>\n",
       "      <td>The Last Empire: Essays 1992-2000</td>\n",
       "      <td>The Last Empire: Essays 1992-2000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>037572639X</td>\n",
       "      <td>037572639X</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Covers_5680</td>\n",
       "      <td>Goodreads_books_30504</td>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0449213943</td>\n",
       "      <td>0449213943</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Covers_7917</td>\n",
       "      <td>Goodreads_books_12253</td>\n",
       "      <td>To See You Again: A True Story of Love in a Ti...</td>\n",
       "      <td>To See You Again: A True Story of Love in a Ti...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0452280710</td>\n",
       "      <td>0452280710</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Covers_8809</td>\n",
       "      <td>Goodreads_books_30883</td>\n",
       "      <td>Drums of Autumn</td>\n",
       "      <td>Drums of Autumn</td>\n",
       "      <td>1.0</td>\n",
       "      <td>044022425X</td>\n",
       "      <td>0525618732</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Covers_7329</td>\n",
       "      <td>Goodreads_books_4049</td>\n",
       "      <td>The Castle in the Attic</td>\n",
       "      <td>The Castle in the Attic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0440409411</td>\n",
       "      <td>0440409411</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     covers_id           goodreads_id  \\\n",
       "0  Covers_4541  Goodreads_books_24907   \n",
       "1  Covers_5680  Goodreads_books_30504   \n",
       "2  Covers_7917  Goodreads_books_12253   \n",
       "3  Covers_8809  Goodreads_books_30883   \n",
       "4  Covers_7329   Goodreads_books_4049   \n",
       "\n",
       "                                        covers_title  \\\n",
       "0                  The Last Empire: Essays 1992-2000   \n",
       "1                     All Quiet on the Western Front   \n",
       "2  To See You Again: A True Story of Love in a Ti...   \n",
       "3                                    Drums of Autumn   \n",
       "4                            The Castle in the Attic   \n",
       "\n",
       "                                     goodreads_title  similarity_score  \\\n",
       "0                  The Last Empire: Essays 1992-2000               1.0   \n",
       "1                     All Quiet on the Western Front               1.0   \n",
       "2  To See You Again: A True Story of Love in a Ti...               1.0   \n",
       "3                                    Drums of Autumn               1.0   \n",
       "4                            The Castle in the Attic               1.0   \n",
       "\n",
       "  covers_isbn goodreads_isbn  is_match  \n",
       "0  037572639X     037572639X      True  \n",
       "1  0449213943     0449213943      True  \n",
       "2  0452280710     0452280710      True  \n",
       "3  044022425X     0525618732     False  \n",
       "4  0440409411     0440409411      True  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleansed datasets\n",
    "covers_df = pd.read_csv(\"data/results/covers_clean_filtered.csv\")\n",
    "goodreads_df = pd.read_csv(\"data/results/goodreads_clean.csv\")\n",
    "covers_df[\"id\"] = 'Covers_' + covers_df.index.astype(str)\n",
    "goodreads_df['id'] = 'Goodreads_books_' + goodreads_df.index.astype(str)\n",
    "\n",
    "# Preprocessing the title columns\n",
    "covers_titles = covers_df['Book-Title']\n",
    "goodreads_titles = goodreads_df['title']\n",
    "\n",
    "# Combining the titles from both datasets for TF-IDF vectorization\n",
    "combined_titles = pd.concat([covers_titles, goodreads_titles])\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(combined_titles)\n",
    "\n",
    "# Splitting the TF-IDF matrix back into two separate matrices for Amazon and Goodreads\n",
    "split_index = len(covers_titles)\n",
    "tfidf_covers = tfidf_matrix[:split_index]\n",
    "tfidf_goodreads = tfidf_matrix[split_index:]\n",
    "\n",
    "# Calculating the cosine similarity between every Amazon title and Goodreads title\n",
    "cosine_similarities = cosine_similarity(tfidf_covers, tfidf_goodreads)\n",
    "\n",
    "# Function to find the best match for each Amazon title in the Goodreads dataset\n",
    "def find_best_matches(cosine_similarities, threshold=0.0):\n",
    "    match_results = []\n",
    "\n",
    "    for i, row in enumerate(cosine_similarities):\n",
    "        best_match_index = np.argmax(row)\n",
    "        best_match_score = row[best_match_index]\n",
    "\n",
    "        if best_match_score > threshold:\n",
    "            covers_id = covers_df.iloc[i]['id']  # Get Covers ID\n",
    "            goodreads_id = goodreads_df.iloc[best_match_index]['id']  # Get Goodreads ID\n",
    "            covers_isbn = covers_df.iloc[i]['ISBN']\n",
    "            goodreads_isbn = goodreads_df.iloc[best_match_index]['isbn']\n",
    "            is_match = covers_isbn == goodreads_isbn\n",
    "            match_results.append({\n",
    "                'covers_id': covers_id,\n",
    "                'goodreads_id': goodreads_id,\n",
    "                'covers_title': covers_df.iloc[i]['Book-Title'],\n",
    "                'goodreads_title': goodreads_df.iloc[best_match_index]['title'],\n",
    "                'similarity_score': best_match_score,\n",
    "                'covers_isbn': covers_isbn,\n",
    "                'goodreads_isbn': goodreads_isbn,\n",
    "                'is_match': is_match\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(match_results)\n",
    "\n",
    "# Finding the best matches\n",
    "matched_df = find_best_matches(cosine_similarities)\n",
    "\n",
    "# Filtering to keep only the highest similarity score match for each ISBN\n",
    "final_matched_covers_goodreads = matched_df.sort_values(by='similarity_score', ascending=False)\\\n",
    "                            .drop_duplicates(subset=['covers_isbn', 'goodreads_isbn'])\\\n",
    "                            .reset_index(drop=True)\n",
    "\n",
    "final_matched_covers_goodreads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>covers_id</th>\n",
       "      <th>amazon_id</th>\n",
       "      <th>covers_title</th>\n",
       "      <th>amazon_title</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>covers_isbn</th>\n",
       "      <th>amazon_isbn</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Covers_8280</td>\n",
       "      <td>Amazon_1115</td>\n",
       "      <td>THE ORIGIN OF SPECIES</td>\n",
       "      <td>The Origin of Species</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9781853267802</td>\n",
       "      <td>9781853267802</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Covers_7636</td>\n",
       "      <td>Amazon_13606</td>\n",
       "      <td>The Only Astrology Book You'll Ever Need</td>\n",
       "      <td>The Only Astrology Book You'll Ever Need</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9781568332314</td>\n",
       "      <td>9781589796539</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Covers_8841</td>\n",
       "      <td>Amazon_11768</td>\n",
       "      <td>On the Banks of Plum Creek</td>\n",
       "      <td>On the Banks of Plum Creek</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9780064400046</td>\n",
       "      <td>9780064400046</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Covers_33306</td>\n",
       "      <td>Amazon_1166</td>\n",
       "      <td>A Briefer History of Time</td>\n",
       "      <td>A Briefer History of Time</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9780716733898</td>\n",
       "      <td>9780593056974</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Covers_7685</td>\n",
       "      <td>Amazon_16408</td>\n",
       "      <td>The Ragamuffin Gospel</td>\n",
       "      <td>The Ragamuffin Gospel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9781576737163</td>\n",
       "      <td>9781590525029</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      covers_id     amazon_id                              covers_title  \\\n",
       "0   Covers_8280   Amazon_1115                     THE ORIGIN OF SPECIES   \n",
       "1   Covers_7636  Amazon_13606  The Only Astrology Book You'll Ever Need   \n",
       "2   Covers_8841  Amazon_11768                On the Banks of Plum Creek   \n",
       "3  Covers_33306   Amazon_1166                 A Briefer History of Time   \n",
       "4   Covers_7685  Amazon_16408                     The Ragamuffin Gospel   \n",
       "\n",
       "                               amazon_title  similarity_score    covers_isbn  \\\n",
       "0                     The Origin of Species               1.0  9781853267802   \n",
       "1  The Only Astrology Book You'll Ever Need               1.0  9781568332314   \n",
       "2                On the Banks of Plum Creek               1.0  9780064400046   \n",
       "3                 A Briefer History of Time               1.0  9780716733898   \n",
       "4                     The Ragamuffin Gospel               1.0  9781576737163   \n",
       "\n",
       "     amazon_isbn  is_match  \n",
       "0  9781853267802      True  \n",
       "1  9781589796539     False  \n",
       "2  9780064400046      True  \n",
       "3  9780593056974     False  \n",
       "4  9781590525029     False  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing the title columns\n",
    "covers_titles = covers_df['Book-Title']\n",
    "amazon_titles = amazon_df['name']\n",
    "\n",
    "# Combining the titles from both datasets for TF-IDF vectorization\n",
    "combined_titles = pd.concat([covers_titles, amazon_titles])\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(combined_titles)\n",
    "\n",
    "# Splitting the TF-IDF matrix back into two separate matrices for Amazon and Covers\n",
    "split_index = len(covers_titles)\n",
    "tfidf_covers = tfidf_matrix[:split_index]\n",
    "tfidf_amazon = tfidf_matrix[split_index:]\n",
    "\n",
    "# Calculating the cosine similarity between every Amazon title and Covers title\n",
    "cosine_similarities = cosine_similarity(tfidf_covers, tfidf_amazon)\n",
    "\n",
    "# Function to find the best match for each Amazon title in the Covers dataset\n",
    "def find_best_matches(cosine_similarities, threshold=0.0):\n",
    "    match_results = []\n",
    "\n",
    "    for i, row in enumerate(cosine_similarities):\n",
    "        best_match_index = np.argmax(row)\n",
    "        best_match_score = row[best_match_index]\n",
    "\n",
    "        if best_match_score > threshold:\n",
    "            covers_id = covers_df.iloc[i]['id']  # Get Covers ID\n",
    "            amazon_id = amazon_df.iloc[best_match_index]['id']  # Get Goodreads ID\n",
    "            covers_isbn = covers_df.iloc[i]['isbn13']\n",
    "            amazon_isbn = amazon_df.iloc[best_match_index]['isbn']\n",
    "            is_match = covers_isbn == amazon_isbn\n",
    "            match_results.append({\n",
    "                'covers_id': covers_id,\n",
    "                'amazon_id': amazon_id,\n",
    "                'covers_title': covers_df.iloc[i]['Book-Title'],\n",
    "                'amazon_title': amazon_df.iloc[best_match_index]['name'],\n",
    "                'similarity_score': best_match_score,\n",
    "                'covers_isbn': covers_isbn,\n",
    "                'amazon_isbn': amazon_isbn,\n",
    "                'is_match': is_match\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(match_results)\n",
    "\n",
    "# Finding the best matches\n",
    "matched_df = find_best_matches(cosine_similarities)\n",
    "\n",
    "# Filtering to keep only the highest similarity score match for each ISBN\n",
    "final_matched_covers_amazon = matched_df.sort_values(by='similarity_score', ascending=False)\\\n",
    "                            .drop_duplicates(subset=['covers_isbn', 'amazon_isbn'])\\\n",
    "                            .reset_index(drop=True)\n",
    "\n",
    "final_matched_covers_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>covers_id</th>\n",
       "      <th>amazon_id</th>\n",
       "      <th>covers_title</th>\n",
       "      <th>amazon_title</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>covers_isbn</th>\n",
       "      <th>amazon_isbn</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Covers_8280</td>\n",
       "      <td>Amazon_1115</td>\n",
       "      <td>THE ORIGIN OF SPECIES</td>\n",
       "      <td>The Origin of Species</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9781853267802</td>\n",
       "      <td>9781853267802</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Covers_7636</td>\n",
       "      <td>Amazon_13606</td>\n",
       "      <td>The Only Astrology Book You'll Ever Need</td>\n",
       "      <td>The Only Astrology Book You'll Ever Need</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9781568332314</td>\n",
       "      <td>9781589796539</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Covers_8841</td>\n",
       "      <td>Amazon_11768</td>\n",
       "      <td>On the Banks of Plum Creek</td>\n",
       "      <td>On the Banks of Plum Creek</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9780064400046</td>\n",
       "      <td>9780064400046</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Covers_33306</td>\n",
       "      <td>Amazon_1166</td>\n",
       "      <td>A Briefer History of Time</td>\n",
       "      <td>A Briefer History of Time</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9780716733898</td>\n",
       "      <td>9780593056974</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Covers_7685</td>\n",
       "      <td>Amazon_16408</td>\n",
       "      <td>The Ragamuffin Gospel</td>\n",
       "      <td>The Ragamuffin Gospel</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9781576737163</td>\n",
       "      <td>9781590525029</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47402</th>\n",
       "      <td>Covers_29036</td>\n",
       "      <td>Amazon_19792</td>\n",
       "      <td>The Necromancer (Necromancer)</td>\n",
       "      <td>The Story of the World</td>\n",
       "      <td>0.042533</td>\n",
       "      <td>9781871438208</td>\n",
       "      <td>9781933339054</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47403</th>\n",
       "      <td>Covers_2995</td>\n",
       "      <td>Amazon_19792</td>\n",
       "      <td>Lando : The Sacketts (Sacketts)</td>\n",
       "      <td>The Story of the World</td>\n",
       "      <td>0.040197</td>\n",
       "      <td>9780553276763</td>\n",
       "      <td>9781933339054</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47404</th>\n",
       "      <td>Covers_33993</td>\n",
       "      <td>Amazon_19792</td>\n",
       "      <td>Tarzan the Untamed (Tarzan)</td>\n",
       "      <td>The Story of the World</td>\n",
       "      <td>0.039005</td>\n",
       "      <td>9780345288684</td>\n",
       "      <td>9781933339054</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47405</th>\n",
       "      <td>Covers_3000</td>\n",
       "      <td>Amazon_19792</td>\n",
       "      <td>Jubal Sackett : The Sacketts (Sacketts)</td>\n",
       "      <td>The Story of the World</td>\n",
       "      <td>0.036617</td>\n",
       "      <td>9780553277395</td>\n",
       "      <td>9781933339054</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47406</th>\n",
       "      <td>Covers_45805</td>\n",
       "      <td>Amazon_19792</td>\n",
       "      <td>The Toyotomi Blades (Toyotomi Blades)</td>\n",
       "      <td>The Story of the World</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>9780312966676</td>\n",
       "      <td>9781933339054</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47407 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          covers_id     amazon_id                              covers_title  \\\n",
       "0       Covers_8280   Amazon_1115                     THE ORIGIN OF SPECIES   \n",
       "1       Covers_7636  Amazon_13606  The Only Astrology Book You'll Ever Need   \n",
       "2       Covers_8841  Amazon_11768                On the Banks of Plum Creek   \n",
       "3      Covers_33306   Amazon_1166                 A Briefer History of Time   \n",
       "4       Covers_7685  Amazon_16408                     The Ragamuffin Gospel   \n",
       "...             ...           ...                                       ...   \n",
       "47402  Covers_29036  Amazon_19792             The Necromancer (Necromancer)   \n",
       "47403   Covers_2995  Amazon_19792           Lando : The Sacketts (Sacketts)   \n",
       "47404  Covers_33993  Amazon_19792               Tarzan the Untamed (Tarzan)   \n",
       "47405   Covers_3000  Amazon_19792   Jubal Sackett : The Sacketts (Sacketts)   \n",
       "47406  Covers_45805  Amazon_19792     The Toyotomi Blades (Toyotomi Blades)   \n",
       "\n",
       "                                   amazon_title  similarity_score  \\\n",
       "0                         The Origin of Species          1.000000   \n",
       "1      The Only Astrology Book You'll Ever Need          1.000000   \n",
       "2                    On the Banks of Plum Creek          1.000000   \n",
       "3                     A Briefer History of Time          1.000000   \n",
       "4                         The Ragamuffin Gospel          1.000000   \n",
       "...                                         ...               ...   \n",
       "47402                    The Story of the World          0.042533   \n",
       "47403                    The Story of the World          0.040197   \n",
       "47404                    The Story of the World          0.039005   \n",
       "47405                    The Story of the World          0.036617   \n",
       "47406                    The Story of the World          0.030492   \n",
       "\n",
       "         covers_isbn    amazon_isbn  is_match  \n",
       "0      9781853267802  9781853267802      True  \n",
       "1      9781568332314  9781589796539     False  \n",
       "2      9780064400046  9780064400046      True  \n",
       "3      9780716733898  9780593056974     False  \n",
       "4      9781576737163  9781590525029     False  \n",
       "...              ...            ...       ...  \n",
       "47402  9781871438208  9781933339054     False  \n",
       "47403  9780553276763  9781933339054     False  \n",
       "47404  9780345288684  9781933339054     False  \n",
       "47405  9780553277395  9781933339054     False  \n",
       "47406  9780312966676  9781933339054     False  \n",
       "\n",
       "[47407 rows x 8 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_matched_goodreads_amazon = final_matched_df\n",
    "final_matched_covers_goodreads\n",
    "final_matched_covers_amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   covers_id         500 non-null    object \n",
      " 1   goodreads_id      500 non-null    object \n",
      " 2   covers_title      500 non-null    object \n",
      " 3   goodreads_title   500 non-null    object \n",
      " 4   similarity_score  500 non-null    float64\n",
      " 5   covers_isbn       500 non-null    object \n",
      " 6   goodreads_isbn    500 non-null    object \n",
      " 7   is_match          500 non-null    bool   \n",
      "dtypes: bool(1), float64(1), object(6)\n",
      "memory usage: 28.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 20 % Matches randomly chosen (100)\n",
    "# 50% Non-Matches randomly chosen (250)\n",
    "# 15% Corner Case Matches (75)\n",
    "# 15% Corner Case Non-Matches (75)\n",
    "\n",
    "# Filter the DataFrame to include only rows where is_match is True\n",
    "matched_df = final_matched_covers_goodreads[(final_matched_covers_goodreads['is_match'] == True) & (final_matched_covers_goodreads['similarity_score'] == 1)]\n",
    "# Filter the DataFrame to include only rows where is_match is False\n",
    "non_matched_df = final_matched_covers_goodreads[(final_matched_covers_goodreads['is_match'] == False) & (final_matched_covers_goodreads['similarity_score'] <= 0.3)]\n",
    "# Define corner cases: matches with a sim score between 0.5 and 0.75\n",
    "corner_cases_match_df = final_matched_covers_goodreads[(final_matched_covers_goodreads['is_match'] == True) & (final_matched_covers_goodreads['similarity_score'] >= 0.65) & (final_matched_covers_goodreads['similarity_score'] <= 0.8)]\n",
    "# Define corner cases for non-matches: assuming non-matches with sim score close to the match threshold\n",
    "corner_cases_non_matches_df = final_matched_covers_goodreads[(final_matched_covers_goodreads['is_match'] == False) & (final_matched_covers_goodreads['similarity_score'] < 0.9) & (final_matched_covers_goodreads['similarity_score'] > 0.7)]\n",
    "\n",
    "\n",
    "# Randomly select 75 corner case entries\n",
    "gs_matched_corner = corner_cases_match_df.sample(n=75, random_state=0)\n",
    "# # Remove corner cases from the matched_df to avoid overlap\n",
    "# matched_df_remaining = matched_df.drop(gs_matched_corner.index)\n",
    "# Randomly select 100 entries from the remaining matched DataFrame\n",
    "gs_matched = matched_df.sample(n=100, random_state=0)\n",
    "\n",
    "# Randomly select 75 corner case non-matched entries\n",
    "gs_non_matched_corner = corner_cases_non_matches_df.sample(n=75, random_state=0)\n",
    "# # Remove corner cases from the non_matched_df to avoid overlap\n",
    "# non_matched_df_remaining = non_matched_df.drop(gs_non_matched_corner.index)\n",
    "# Randomly select 250 entries from the remaining non-matched DataFrame\n",
    "gs_non_matched = non_matched_df.sample(n=250, random_state=0)\n",
    "\n",
    "# Concatenate all four groups into one DataFrame\n",
    "gs_combined = pd.concat([gs_matched, gs_non_matched, gs_matched_corner, gs_non_matched_corner])\n",
    "\n",
    "# Reset the index of the combined DataFrame\n",
    "gs_combined = gs_combined.reset_index(drop=True)\n",
    "\n",
    "gs_combined.to_csv(\"gs_covers_goodreads.csv\")\n",
    "\n",
    "# Verify the combined DataFrame\n",
    "print(gs_combined.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   amazon_id         500 non-null    object \n",
      " 1   goodreads_id      500 non-null    object \n",
      " 2   amazon_title      500 non-null    object \n",
      " 3   goodreads_title   500 non-null    object \n",
      " 4   similarity_score  500 non-null    float64\n",
      " 5   amazon_isbn       500 non-null    int64  \n",
      " 6   goodreads_isbn    500 non-null    int64  \n",
      " 7   is_match          500 non-null    bool   \n",
      "dtypes: bool(1), float64(1), int64(2), object(4)\n",
      "memory usage: 28.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 20 % Matches randomly chosen (100)\n",
    "# 50% Non-Matches randomly chosen (250)\n",
    "# 15% Corner Case Matches (75)\n",
    "# 15% Corner Case Non-Matches (75)\n",
    "\n",
    "# Filter the DataFrame to include only rows where is_match is True\n",
    "matched_df = final_matched_goodreads_amazon[(final_matched_goodreads_amazon['is_match'] == True) & (final_matched_goodreads_amazon['similarity_score'] == 1)]\n",
    "# Filter the DataFrame to include only rows where is_match is False\n",
    "non_matched_df = final_matched_goodreads_amazon[(final_matched_goodreads_amazon['is_match'] == False) & (final_matched_goodreads_amazon['similarity_score'] <= 0.3)]\n",
    "# Define corner cases: matches with a sim score between 0.5 and 0.75\n",
    "corner_cases_match_df = final_matched_goodreads_amazon[(final_matched_goodreads_amazon['is_match'] == True) & (final_matched_goodreads_amazon['similarity_score'] >= 0.65) & (final_matched_goodreads_amazon['similarity_score'] <= 0.8)]\n",
    "# Define corner cases for non-matches: assuming non-matches with sim score close to the match threshold\n",
    "corner_cases_non_matches_df = final_matched_goodreads_amazon[(final_matched_goodreads_amazon['is_match'] == False) & (final_matched_goodreads_amazon['similarity_score'] < 0.95) & (final_matched_goodreads_amazon['similarity_score'] > 0.7)]\n",
    "\n",
    "\n",
    "# Randomly select 75 corner case entries\n",
    "gs_matched_corner = corner_cases_match_df.sample(n=75, random_state=0)\n",
    "# # Remove corner cases from the matched_df to avoid overlap\n",
    "# matched_df_remaining = matched_df.drop(gs_matched_corner.index)\n",
    "# Randomly select 100 entries from the remaining matched DataFrame\n",
    "gs_matched = matched_df.sample(n=100, random_state=0)\n",
    "\n",
    "# Randomly select 75 corner case non-matched entries\n",
    "gs_non_matched_corner = corner_cases_non_matches_df.sample(n=75, random_state=0)\n",
    "# # Remove corner cases from the non_matched_df to avoid overlap\n",
    "# non_matched_df_remaining = non_matched_df.drop(gs_non_matched_corner.index)\n",
    "# Randomly select 250 entries from the remaining non-matched DataFrame\n",
    "gs_non_matched = non_matched_df.sample(n=250, random_state=0)\n",
    "\n",
    "# Concatenate all four groups into one DataFrame\n",
    "gs_combined = pd.concat([gs_matched, gs_non_matched, gs_matched_corner, gs_non_matched_corner])\n",
    "\n",
    "# Reset the index of the combined DataFrame\n",
    "gs_combined = gs_combined.reset_index(drop=True)\n",
    "\n",
    "gs_combined.to_csv(\"gs_goodreads_amazon.csv\")\n",
    "\n",
    "# Verify the combined DataFrame\n",
    "print(gs_combined.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   covers_id         500 non-null    object \n",
      " 1   amazon_id         500 non-null    object \n",
      " 2   covers_title      500 non-null    object \n",
      " 3   amazon_title      500 non-null    object \n",
      " 4   similarity_score  500 non-null    float64\n",
      " 5   covers_isbn       500 non-null    int64  \n",
      " 6   amazon_isbn       500 non-null    int64  \n",
      " 7   is_match          500 non-null    bool   \n",
      "dtypes: bool(1), float64(1), int64(2), object(4)\n",
      "memory usage: 28.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 20 % Matches randomly chosen (100)\n",
    "# 50% Non-Matches randomly chosen (250)\n",
    "# 15% Corner Case Matches (75)\n",
    "# 15% Corner Case Non-Matches (75)\n",
    "\n",
    "# Filter the DataFrame to include only rows where is_match is True\n",
    "matched_df = final_matched_covers_amazon[(final_matched_covers_amazon['is_match'] == True) & (final_matched_covers_amazon['similarity_score'] == 1)]\n",
    "# Filter the DataFrame to include only rows where is_match is False\n",
    "non_matched_df = final_matched_covers_amazon[(final_matched_covers_amazon['is_match'] == False) & (final_matched_covers_amazon['similarity_score'] <= 0.3)]\n",
    "# Define corner cases: matches with a sim score between 0.5 and 0.75\n",
    "corner_cases_match_df = final_matched_covers_amazon[(final_matched_covers_amazon['is_match'] == True) & (final_matched_covers_amazon['similarity_score'] >= 0.65) & (final_matched_covers_amazon['similarity_score'] <= 0.8)]\n",
    "# Define corner cases for non-matches: assuming non-matches with sim score close to the match threshold\n",
    "corner_cases_non_matches_df = final_matched_covers_amazon[(final_matched_covers_amazon['is_match'] == False) & (final_matched_covers_amazon['similarity_score'] < 0.95) & (final_matched_covers_amazon['similarity_score'] > 0.7)]\n",
    "\n",
    "\n",
    "# Randomly select 75 corner case entries\n",
    "gs_matched_corner = corner_cases_match_df.sample(n=75, random_state=0)\n",
    "# # Remove corner cases from the matched_df to avoid overlap\n",
    "# matched_df_remaining = matched_df.drop(gs_matched_corner.index)\n",
    "# Randomly select 100 entries from the remaining matched DataFrame\n",
    "gs_matched = matched_df.sample(n=100, random_state=0)\n",
    "\n",
    "# Randomly select 75 corner case non-matched entries\n",
    "gs_non_matched_corner = corner_cases_non_matches_df.sample(n=75, random_state=0)\n",
    "# # Remove corner cases from the non_matched_df to avoid overlap\n",
    "# non_matched_df_remaining = non_matched_df.drop(gs_non_matched_corner.index)\n",
    "# Randomly select 250 entries from the remaining non-matched DataFrame\n",
    "gs_non_matched = non_matched_df.sample(n=250, random_state=0)\n",
    "\n",
    "# Concatenate all four groups into one DataFrame\n",
    "gs_combined = pd.concat([gs_matched, gs_non_matched, gs_matched_corner, gs_non_matched_corner])\n",
    "\n",
    "# Reset the index of the combined DataFrame\n",
    "gs_combined = gs_combined.reset_index(drop=True)\n",
    "\n",
    "gs_combined.to_csv(\"gs_covers_amazon.csv\")\n",
    "\n",
    "# Verify the combined DataFrame\n",
    "print(gs_combined.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
